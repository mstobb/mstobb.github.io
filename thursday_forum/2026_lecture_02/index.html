<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 02: How do Large Language Models work? | Michael T. Stobb</title><meta name=keywords content><meta name=description content="This page contains notes, further readings, and general references for the associated lecture.
The Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.
If you&rsquo;re looking for notes to the other lectures, head here.
More about Perceptrons
While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber 3Blue1Brown. The entire series is worth a watch, but be warned, it&rsquo;s quite long."><meta name=author content><link rel=canonical href=https://stobb.org/thursday_forum/2026_lecture_02/><link crossorigin=anonymous href=/assets/css/stylesheet.960f1449777f1c27112a84b443014ca324d51065a68183f9694cf5e1f9f1f49a.css integrity="sha256-lg8USXd/HCcRKoS0QwFMoyTVEGWmgYP5aUz14fnx9Jo=" rel="preload stylesheet" as=style><link rel=icon href=https://stobb.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://stobb.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://stobb.org/favicon-32x32.png><link rel=apple-touch-icon href=https://stobb.org/apple-touch-icon.png><link rel=mask-icon href=https://stobb.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://stobb.org/thursday_forum/2026_lecture_02/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><meta property="og:url" content="https://stobb.org/thursday_forum/2026_lecture_02/"><meta property="og:site_name" content="Michael T. Stobb"><meta property="og:title" content="Lecture 02: How do Large Language Models work?"><meta property="og:description" content="This page contains notes, further readings, and general references for the associated lecture.
The Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.
If you’re looking for notes to the other lectures, head here.
More about Perceptrons While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber 3Blue1Brown. The entire series is worth a watch, but be warned, it’s quite long."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="thursday_forum"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lecture 02: How do Large Language Models work?"><meta name=twitter:description content="This page contains notes, further readings, and general references for the associated lecture.
The Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.
If you&rsquo;re looking for notes to the other lectures, head here.
More about Perceptrons
While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber 3Blue1Brown. The entire series is worth a watch, but be warned, it&rsquo;s quite long."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"2026 Thursday Forum References and Notes","item":"https://stobb.org/thursday_forum/"},{"@type":"ListItem","position":2,"name":"Lecture 02: How do Large Language Models work?","item":"https://stobb.org/thursday_forum/2026_lecture_02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 02: How do Large Language Models work?","name":"Lecture 02: How do Large Language Models work?","description":"This page contains notes, further readings, and general references for the associated lecture.\nThe Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.\nIf you\u0026rsquo;re looking for notes to the other lectures, head here.\nMore about Perceptrons While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber 3Blue1Brown. The entire series is worth a watch, but be warned, it\u0026rsquo;s quite long.\n","keywords":[],"articleBody":"This page contains notes, further readings, and general references for the associated lecture.\nThe Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.\nIf you’re looking for notes to the other lectures, head here.\nMore about Perceptrons While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber 3Blue1Brown. The entire series is worth a watch, but be warned, it’s quite long.\nDuring the lecture we showed the awesome 2D simulations for a Multilayered Perceptron digit recognizer created by Adam Harley. There are a number of other visualizations that Adam has made for alternative neural network structures (ones we did not discuss in our lecture) that are also great to see.\nOne important neural network structure we did not have time to discuss but is still highly used in modern systems is the convolutional neural network. Backpropagation This is some of the most technical stuff we covered, and likely some of the most technical stuff involved in current AI models. It cannot be understated how imporant backpropagation is to both the modern models we are discussing (e.g., ChatGPT), but also to lots of regular “narrow AI” models of the last 40 years. It is the single most important component in the entire stack as it fundamentally allows an impossibly complex model to slowly get better through exposure to examples. Note, if you want to really understand how it works, you will have to get into some calculus.\nHere are some resources if you want to dive into it a little deeper:\n3Blue1Brown’s video on backpropagation is just visually amazing. Highly recommended. The Builtin article on backprop is also good (and even cites the above video) and gives some additional background The people at Welch Labs have also created a great video series on backpropagation (be sure to see parts 2 and 3 also) which gets a bit more technical, but well worth a watch Words as Numbers Turning words into numbers (in lecture we thought of them as barcodes) has many applications, beyond even LLMs. It’s been a pretty standard task in the AI world for the last 30 years, meaning there’s lots of different ways to do it.\nThis video by Josh Starmer on word embeddings is a little silly (most of his videos are), but the mathematics and explanations are highly accurate IBM also put out a very decent article on how these numbers are created and used Transformers We absolutely did not cover this in depth. Honestly, we barely mentioned it. And while it’s not the largest component of modern AI systems (that’s still the perceptron we spent so much time talking about) it’s the component that makes the whole thing work. They were first presented famously in a Google research paper Attention is All You Need, a naming scheme that has continued to be reused since then. It’s a techinical paper and not for an introductory crowd, but it still manages to be quite readable.\nFor a more visual explanation of a Transformer, see the PoloClub interactive tutorial. It too is pretty technical, but also beautiful. Transformers have two main components: Encoders and Decoders. Josh Starmer from StatQuest does a decent (but silly) job with both of them (encoders, decoders). Large Language Models A classic visualization of ChatGPT (version 3, so an older version) is available by Jay Alammar. It’s a little older and not quite accurate for the truly modern models, but its still great for understanding the basics of full system.\nFull citation for the “Tale of Two Cities” used in the lecture\nAndreas Stöffelbauer wrote a very detailed article on the basics of LLMs\nTraining Modern AI Models Stephen Bach wrote a pretty good article describing the different phases of AI training. Well worth a quick read through. Josh Starmer (from StatQuest) created a good video explaining Reinforcement Learning with Human Feedback (RLHF). His videos are always a bit silly, but they are quite approachable. Neptune.ai has a very good and very understandable article on RHLF as well AI Benchmarks Aspen Digital put together a great primer on AI benchmarks, a good place to get started on learning more. Stanford’s Holistic Evaluation of Language Models (HELM) maintains a nice leaderboard for various AI benchmarks HuggingFace (an opensource AI site for sharing models) also maintains a leaderboard for AI benchmarks The specific MMMLU leaderboard shown during the lecture can be found here The original paper on MMLU is publicly available as well. Humanity’s Last Exam has it’s own webpage for information on it, with links to the official paper. As does ARC-AGI-1 and ARC-AGI-2 Nature Paper Finally, the very recent Nature paper detailing why current LLMs should be classified as being “artifical general intelligence”. It’s a good article from one of the most reputable journals in all academia. Interestingly, these same ideas have been stated by less prestigous individuals and organizations for at least the last 6 months, if not longer, but it’s good to see it out in Nature now.\n","wordCount":"872","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://stobb.org/thursday_forum/2026_lecture_02/"},"publisher":{"@type":"Organization","name":"Michael T. Stobb","logo":{"@type":"ImageObject","url":"https://stobb.org/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://stobb.org/ accesskey=h title="Michael T. Stobb (Alt + H)">Michael T. Stobb</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://stobb.org/about/ title=About><span>About</span></a></li><li><a href=https://stobb.org/research/ title=Research><span>Research</span></a></li><li><a href=https://stobb.org/teaching/ title=Teaching><span>Teaching</span></a></li><li><a href=https://stobb.org/software/ title=Software><span>Software</span></a></li><li><a href=https://stobb.org/personal/ title=Personal><span>Personal</span></a></li><li><a href=https://stobb.org/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Lecture 02: How do Large Language Models work?</h1></header><div class=post-content><p>This page contains notes, further readings, and general references for the associated lecture.</p><p>The <a href="https://docs.google.com/presentation/d/e/2PACX-1vRrzQcgdPw7pqnBPIb9xKzEOC3oJ2ounvZsk7lVo2pDhTbAJazZ63-KFlPVNt8_UgBgrw-GA-aUvfoD/pub?start=false&amp;loop=false&amp;delayms=10000">Lecture Slides</a> for the talk are also available (use your arrow keys to advance the slides) or download a <a href=Stobb_ThursdayForum_2026_Lecture_02.pdf>pdf of the slides</a>.</p><p>If you&rsquo;re looking for notes to the other lectures, head <a href=/thursday_forum/>here</a>.</p><h2 id=more-about-perceptrons>More about Perceptrons<a hidden class=anchor aria-hidden=true href=#more-about-perceptrons>#</a></h2><p>While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber <a href=https://www.3blue1brown.com/topics/neural-networks>3Blue1Brown</a>. The entire series is worth a watch, but be warned, it&rsquo;s quite long.</p><p>During the lecture we showed the awesome 2D simulations for a Multilayered Perceptron <a href=https://adamharley.com/nn_vis/mlp/2d.html>digit recognizer</a> created by Adam Harley. There are a number of <a href=https://adamharley.com/nn_vis/>other visualizations</a> that Adam has made for alternative neural network structures (ones we did not discuss in our lecture) that are also great to see.</p><ul><li>One important neural network structure we did not have time to discuss but is still highly used in modern systems is the <a href=https://adamharley.com/nn_vis/cnn/2d.html>convolutional</a> neural network.</li></ul><h2 id=backpropagation>Backpropagation<a hidden class=anchor aria-hidden=true href=#backpropagation>#</a></h2><p>This is some of the most technical stuff we covered, and likely some of the most technical stuff involved in current AI models. It cannot be understated how imporant <a href=https://en.wikipedia.org/wiki/Backpropagation>backpropagation</a> is to both the modern models we are discussing (e.g., ChatGPT), but also to lots of regular &ldquo;narrow AI&rdquo; models of the last 40 years. It is the single most important component in the entire stack as it fundamentally allows an impossibly complex model to slowly get better through exposure to examples. Note, if you want to <em>really</em> understand how it works, you will have to get into some calculus.</p><p>Here are some resources if you want to dive into it a little deeper:</p><ul><li>3Blue1Brown&rsquo;s video on <a href="https://www.3blue1brown.com/?v=backpropagation">backpropagation</a> is just visually amazing. Highly recommended.</li><li>The Builtin <a href=https://builtin.com/machine-learning/backpropagation-neural-network>article</a> on backprop is also good (and even cites the above video) and gives some additional background</li><li>The people at Welch Labs have also created a great video series on <a href="https://youtu.be/NrO20Jb-hy0?si=HynA3qYza7LJTN_9">backpropagation</a> (be sure to see parts <a href="https://youtu.be/VkHfRKewkWw?si=k4gVJJEjN1N-HYRz">2</a> and <a href="https://youtu.be/qx7hirqgfuU?si=ZUL_9A9sCiqxx68i">3</a> also) which gets a bit more technical, but well worth a watch</li></ul><h2 id=words-as-numbers>Words as Numbers<a hidden class=anchor aria-hidden=true href=#words-as-numbers>#</a></h2><p>Turning words into numbers (in lecture we thought of them as <em>barcodes</em>) has many applications, beyond even LLMs. It&rsquo;s been a pretty standard task in the AI world for the last 30 years, meaning there&rsquo;s lots of different ways to do it.</p><ul><li>This video by Josh Starmer on <a href="https://youtu.be/viZrOnJclY0?si=ZrLTwN66CJuKhvvz">word embeddings</a> is a little silly (most of his videos are), but the mathematics and explanations are highly accurate</li><li>IBM also put out a very decent <a href=https://www.ibm.com/think/topics/word-embeddings>article</a> on how these numbers are created and used</li></ul><h2 id=transformers>Transformers<a hidden class=anchor aria-hidden=true href=#transformers>#</a></h2><p>We absolutely did not cover this in depth. Honestly, we barely mentioned it. And while it&rsquo;s not the largest component of modern AI systems (that&rsquo;s still the perceptron we spent so much time talking about) it&rsquo;s <strong>the component</strong> that makes the whole thing work. They were first presented famously in a Google research paper <a href=https://arxiv.org/pdf/1706.03762>Attention is All You Need</a>, a naming scheme that has continued to be reused since then. It&rsquo;s a techinical paper and not for an introductory crowd, but it still manages to be quite readable.</p><ul><li>For a more visual explanation of a Transformer, see the <a href=https://poloclub.github.io/transformer-explainer/>PoloClub</a> interactive tutorial. It too is pretty technical, but also beautiful.</li><li>Transformers have two main components: Encoders and Decoders. Josh Starmer from StatQuest does a decent (but silly) job with both of them (<a href="https://youtu.be/GDN649X_acE?si=SEF5-a-0quJ_RGGB">encoders</a>, <a href="https://youtu.be/bQ5BoolX9Ag?si=Jt4JWSZiJADSNf7d">decoders</a>).</li></ul><h2 id=large-language-models>Large Language Models<a hidden class=anchor aria-hidden=true href=#large-language-models>#</a></h2><ul><li><p>A classic visualization of ChatGPT (version 3, so an older version) is available by <a href=https://jalammar.github.io/how-gpt3-works-visualizations-animations/>Jay Alammar</a>. It&rsquo;s a little older and not quite accurate for the truly modern models, but its still great for understanding the basics of full system.</p></li><li><p>Full citation for the <a href=https://www.online-literature.com/dickens/twocities/1/>&ldquo;Tale of Two Cities&rdquo;</a> used in the lecture</p></li><li><p>Andreas Stöffelbauer wrote a very detailed <a href=https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f>article</a> on the basics of LLMs</p></li></ul><h2 id=training-modern-ai-models>Training Modern AI Models<a hidden class=anchor aria-hidden=true href=#training-modern-ai-models>#</a></h2><ul><li>Stephen Bach wrote a pretty good <a href=https://snorkel.ai/blog/large-language-model-training-three-phases-shape-llm-training/>article</a> describing the different phases of AI training. Well worth a quick read through.</li><li>Josh Starmer (from StatQuest) created a good <a href="https://youtu.be/qPN_XZcJf_s?si=_Bi6RSuGWFWWf4fN">video</a> explaining Reinforcement Learning with Human Feedback (RLHF). His videos are always a bit silly, but they are quite approachable.</li><li>Neptune.ai has a very good and very understandable <a href=https://neptune.ai/blog/reinforcement-learning-from-human-feedback-for-llms>article</a> on RHLF as well</li></ul><h2 id=ai-benchmarks>AI Benchmarks<a hidden class=anchor aria-hidden=true href=#ai-benchmarks>#</a></h2><ul><li>Aspen Digital put together a great <a href=https://www.aspendigital.org/report/benchmarks-101/>primer</a> on AI benchmarks, a good place to get started on learning more.</li><li>Stanford&rsquo;s Holistic Evaluation of Language Models (HELM) maintains a nice <a href=https://crfm.stanford.edu/helm/>leaderboard</a> for various AI benchmarks</li><li>HuggingFace (an opensource AI site for sharing models) also maintains a <a href=https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/>leaderboard</a> for AI benchmarks</li><li>The specific MMMLU leaderboard shown during the lecture can be found <a href=https://artificialanalysis.ai/evaluations/mmlu-pro>here</a></li><li>The <a href=https://arxiv.org/pdf/2009.03300>original paper on MMLU</a> is publicly available as well.</li><li>Humanity&rsquo;s Last Exam has it&rsquo;s own <a href=https://agi.safe.ai/>webpage</a> for information on it, with links to the official paper.</li><li>As does <a href=https://arcprize.org/arc-agi/1/>ARC-AGI-1</a> and <a href=https://arcprize.org/arc-agi/2/>ARC-AGI-2</a></li></ul><h2 id=nature-paper>Nature Paper<a hidden class=anchor aria-hidden=true href=#nature-paper>#</a></h2><p>Finally, the <em>very recent</em> <a href=https://www.nature.com/articles/d41586-026-00285-6>Nature paper</a> detailing why current LLMs should be classified as being &ldquo;artifical general intelligence&rdquo;. It&rsquo;s a good article from one of the most reputable journals in all academia. Interestingly, these same ideas have been stated by less prestigous individuals and organizations for at least the last 6 months, if not longer, but it&rsquo;s good to see it out in Nature now.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://stobb.org/>Michael T. Stobb</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>