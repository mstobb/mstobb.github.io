<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="dark">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Lecture 02: How do Large Language Models work? | Michael T. Stobb</title>
<meta name="keywords" content="">
<meta name="description" content="This page contains notes, further readings, and general references for the associated lecture.
The Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.
If you&rsquo;re looking for notes to the other lectures, head here.
More about Perceptrons
While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber 3Blue1Brown. The entire series is worth a watch, but be warned, it&rsquo;s quite long.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/thursday_forum/2026_lecture_02/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.960f1449777f1c27112a84b443014ca324d51065a68183f9694cf5e1f9f1f49a.css" integrity="sha256-lg8USXd/HCcRKoS0QwFMoyTVEGWmgYP5aUz14fnx9Jo=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/thursday_forum/2026_lecture_02/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Michael T. Stobb (Alt + H)">Michael T. Stobb</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/research/" title="Research">
                    <span>Research</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/teaching/" title="Teaching">
                    <span>Teaching</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/software/" title="Software">
                    <span>Software</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/personal/" title="Personal">
                    <span>Personal</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/contact/" title="Contact">
                    <span>Contact</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Lecture 02: How do Large Language Models work?
    </h1>
  </header> 
  <div class="post-content"><p>This page contains notes, further readings, and general references for the associated lecture.</p>
<p>The <a href="https://docs.google.com/presentation/d/e/2PACX-1vRrzQcgdPw7pqnBPIb9xKzEOC3oJ2ounvZsk7lVo2pDhTbAJazZ63-KFlPVNt8_UgBgrw-GA-aUvfoD/pub?start=false&amp;loop=false&amp;delayms=10000">Lecture Slides</a> for the talk are also available (use your arrow keys to advance the slides) or download a <a href="Stobb_ThursdayForum_2026_Lecture_02.pdf">pdf of the slides</a>.</p>
<p>If you&rsquo;re looking for notes to the other lectures, head <a href="/thursday_forum/">here</a>.</p>
<h2 id="more-about-perceptrons">More about Perceptrons<a hidden class="anchor" aria-hidden="true" href="#more-about-perceptrons">#</a></h2>
<p>While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber <a href="https://www.3blue1brown.com/topics/neural-networks">3Blue1Brown</a>. The entire series is worth a watch, but be warned, it&rsquo;s quite long.</p>
<p>During the lecture we showed the awesome 2D simulations for a Multilayered Perceptron <a href="https://adamharley.com/nn_vis/mlp/2d.html">digit recognizer</a> created by Adam Harley. There are a number of <a href="https://adamharley.com/nn_vis/">other visualizations</a> that Adam has made for alternative neural network structures (ones we did not discuss in our lecture) that are also great to see.</p>
<ul>
<li>One important neural network structure we did not have time to discuss but is still highly used in modern systems is the <a href="https://adamharley.com/nn_vis/cnn/2d.html">convolutional</a> neural network.</li>
</ul>
<h2 id="backpropagation">Backpropagation<a hidden class="anchor" aria-hidden="true" href="#backpropagation">#</a></h2>
<p>This is some of the most technical stuff we covered, and likely some of the most technical stuff involved in current AI models. It cannot be understated how imporant <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> is to both the modern models we are discussing (e.g., ChatGPT), but also to lots of regular &ldquo;narrow AI&rdquo; models of the last 40 years. It is the single most important component in the entire stack as it fundamentally allows an impossibly complex model to slowly get better through exposure to examples. Note, if you want to <em>really</em> understand how it works, you will have to get into some calculus.</p>
<p>Here are some resources if you want to dive into it a little deeper:</p>
<ul>
<li>3Blue1Brown&rsquo;s video on <a href="https://www.3blue1brown.com/?v=backpropagation">backpropagation</a> is just visually amazing. Highly recommended.</li>
<li>The Builtin <a href="https://builtin.com/machine-learning/backpropagation-neural-network">article</a> on backprop is also good (and even cites the above video) and gives some additional background</li>
<li>The people at Welch Labs have also created a great video series on <a href="https://youtu.be/NrO20Jb-hy0?si=HynA3qYza7LJTN_9">backpropagation</a> (be sure to see parts <a href="https://youtu.be/VkHfRKewkWw?si=k4gVJJEjN1N-HYRz">2</a> and <a href="https://youtu.be/qx7hirqgfuU?si=ZUL_9A9sCiqxx68i">3</a> also) which gets a bit more technical, but well worth a watch</li>
</ul>
<h2 id="words-as-numbers">Words as Numbers<a hidden class="anchor" aria-hidden="true" href="#words-as-numbers">#</a></h2>
<p>Turning words into numbers (in lecture we thought of them as <em>barcodes</em>) has many applications, beyond even LLMs. It&rsquo;s been a pretty standard task in the AI world for the last 30 years, meaning there&rsquo;s lots of different ways to do it.</p>
<ul>
<li>This video by Josh Starmer on <a href="https://youtu.be/viZrOnJclY0?si=ZrLTwN66CJuKhvvz">word embeddings</a> is a little silly (most of his videos are), but the mathematics and explanations are highly accurate</li>
<li>IBM also put out a very decent <a href="https://www.ibm.com/think/topics/word-embeddings">article</a> on how these numbers are created and used</li>
</ul>
<h2 id="transformers">Transformers<a hidden class="anchor" aria-hidden="true" href="#transformers">#</a></h2>
<p>We absolutely did not cover this in depth. Honestly, we barely mentioned it. And while it&rsquo;s not the largest component of modern AI systems (that&rsquo;s still the perceptron we spent so much time talking about) it&rsquo;s <strong>the component</strong> that makes the whole thing work. They were first presented famously in a Google research paper <a href="https://arxiv.org/pdf/1706.03762">Attention is All You Need</a>, a naming scheme that has continued to be reused since then. It&rsquo;s a techinical paper and not for an introductory crowd, but it still manages to be quite readable.</p>
<ul>
<li>For a more visual explanation of a Transformer, see the <a href="https://poloclub.github.io/transformer-explainer/">PoloClub</a> interactive tutorial. It too is pretty technical, but also beautiful.</li>
<li>Transformers have two main components: Encoders and Decoders. Josh Starmer from StatQuest does a decent (but silly) job with both of them (<a href="https://youtu.be/GDN649X_acE?si=SEF5-a-0quJ_RGGB">encoders</a>, <a href="https://youtu.be/bQ5BoolX9Ag?si=Jt4JWSZiJADSNf7d">decoders</a>).</li>
</ul>
<h2 id="large-language-models">Large Language Models<a hidden class="anchor" aria-hidden="true" href="#large-language-models">#</a></h2>
<ul>
<li>
<p>A classic visualization of ChatGPT (version 3, so an older version) is available by <a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">Jay Alammar</a>. It&rsquo;s a little older and not quite accurate for the truly modern models, but its still great for understanding the basics of full system.</p>
</li>
<li>
<p>Full citation for the <a href="https://www.online-literature.com/dickens/twocities/1/">&ldquo;Tale of Two Cities&rdquo;</a> used in the lecture</p>
</li>
<li>
<p>Andreas Stöffelbauer wrote a very detailed <a href="https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f">article</a> on the basics of LLMs</p>
</li>
</ul>
<h2 id="training-modern-ai-models">Training Modern AI Models<a hidden class="anchor" aria-hidden="true" href="#training-modern-ai-models">#</a></h2>
<ul>
<li>Stephen Bach wrote a pretty good <a href="https://snorkel.ai/blog/large-language-model-training-three-phases-shape-llm-training/">article</a> describing the different phases of AI training. Well worth a quick read through.</li>
<li>Josh Starmer (from StatQuest) created a good <a href="https://youtu.be/qPN_XZcJf_s?si=_Bi6RSuGWFWWf4fN">video</a> explaining Reinforcement Learning with Human Feedback (RLHF). His videos are always a bit silly, but they are quite approachable.</li>
<li>Neptune.ai has a very good and very understandable <a href="https://neptune.ai/blog/reinforcement-learning-from-human-feedback-for-llms">article</a> on RHLF as well</li>
</ul>
<h2 id="ai-benchmarks">AI Benchmarks<a hidden class="anchor" aria-hidden="true" href="#ai-benchmarks">#</a></h2>
<ul>
<li>Aspen Digital put together a great <a href="https://www.aspendigital.org/report/benchmarks-101/">primer</a> on AI benchmarks, a good place to get started on learning more.</li>
<li>Stanford&rsquo;s Holistic Evaluation of Language Models (HELM) maintains a nice <a href="https://crfm.stanford.edu/helm/">leaderboard</a> for various AI benchmarks</li>
<li>HuggingFace (an opensource AI site for sharing models) also maintains a <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/">leaderboard</a> for AI benchmarks</li>
<li>The specific MMMLU leaderboard shown during the lecture can be found <a href="https://artificialanalysis.ai/evaluations/mmlu-pro">here</a></li>
<li>The <a href="https://arxiv.org/pdf/2009.03300">original paper on MMLU</a> is publicly available as well.</li>
<li>Humanity&rsquo;s Last Exam has it&rsquo;s own <a href="https://agi.safe.ai/">webpage</a> for information on it, with links to the official paper.</li>
<li>As does <a href="https://arcprize.org/arc-agi/1/">ARC-AGI-1</a> and <a href="https://arcprize.org/arc-agi/2/">ARC-AGI-2</a></li>
</ul>
<h2 id="nature-paper">Nature Paper<a hidden class="anchor" aria-hidden="true" href="#nature-paper">#</a></h2>
<p>Finally, the <em>very recent</em> <a href="https://www.nature.com/articles/d41586-026-00285-6">Nature paper</a> detailing why current LLMs should be classified as being &ldquo;artifical general intelligence&rdquo;. It&rsquo;s a good article from one of the most reputable journals in all academia. Interestingly, these same ideas have been stated by less prestigous individuals and organizations for at least the last 6 months, if not longer, but it&rsquo;s good to see it out in Nature now.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Michael T. Stobb</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
