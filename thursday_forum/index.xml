<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>2026 Thursday Forum References and Notes on Michael T. Stobb</title><link>https://stobb.org/thursday_forum/</link><description>Recent content in 2026 Thursday Forum References and Notes on Michael T. Stobb</description><generator>Hugo -- 0.155.3</generator><language>en-us</language><lastBuildDate>Thu, 05 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://stobb.org/thursday_forum/index.xml" rel="self" type="application/rss+xml"/><item><title>Lecture 01: How did we get here?</title><link>https://stobb.org/thursday_forum/2026_lecture_01/</link><pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate><guid>https://stobb.org/thursday_forum/2026_lecture_01/</guid><description>&lt;p&gt;This page contains notes, further readings, and general references for the associated lecture.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://docs.google.com/presentation/d/e/2PACX-1vTLaVcNoa7dHYlBRmaX_-6Ynie8RDWHb989P8d8UkfsoOKQzYuYGMlIydZMDcED1MC_ua6ypJwSd8TQ/pub?start=false&amp;amp;loop=false&amp;amp;delayms=60000"&gt;Lecture Slides&lt;/a&gt; for the talk are also available (use your arrow keys to advance the slides) or download a &lt;a href="Stobb_ThursdayForum_2026_Lecture_01.pdf"&gt;pdf of the slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re looking for the other lecture notes, head &lt;a href="https://stobb.org/thursday_forum/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="ai-scaling-laws"&gt;AI Scaling Laws&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A great visualization of the current &lt;a href="https://www.reuters.com/graphics/USA-ECONOMY/AI-INVESTMENT/gkvlqbgxkpb/"&gt;AI investment&lt;/a&gt; is available by Reuters. Be sure to keep scrolling through the page to get a real sense for the current size of the AI investment.&lt;/p&gt;</description></item><item><title>Lecture 02: How do Large Language Models work?</title><link>https://stobb.org/thursday_forum/2026_lecture_02/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://stobb.org/thursday_forum/2026_lecture_02/</guid><description>&lt;p&gt;This page contains notes, further readings, and general references for the associated lecture.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://docs.google.com/presentation/d/e/2PACX-1vRrzQcgdPw7pqnBPIb9xKzEOC3oJ2ounvZsk7lVo2pDhTbAJazZ63-KFlPVNt8_UgBgrw-GA-aUvfoD/pub?start=false&amp;amp;loop=false&amp;amp;delayms=10000"&gt;Lecture Slides&lt;/a&gt; for the talk are also available (use your arrow keys to advance the slides) or download a &lt;a href="Stobb_ThursdayForum_2026_Lecture_02.pdf"&gt;pdf of the slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re looking for notes to the other lectures, head &lt;a href="https://stobb.org/thursday_forum/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="more-about-perceptrons"&gt;More about Perceptrons&lt;/h2&gt;
&lt;p&gt;While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber &lt;a href="https://www.3blue1brown.com/topics/neural-networks"&gt;3Blue1Brown&lt;/a&gt;. The entire series is worth a watch, but be warned, it&amp;rsquo;s quite long.&lt;/p&gt;</description></item></channel></rss>