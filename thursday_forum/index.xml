<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>2026 Thursday Forum References and Notes on Michael T. Stobb</title><link>https://stobb.org/thursday_forum/</link><description>Recent content in 2026 Thursday Forum References and Notes on Michael T. Stobb</description><generator>Hugo -- 0.157.0</generator><language>en-us</language><managingEditor>michael@stobb.org (Michael Stobb)</managingEditor><webMaster>michael@stobb.org (Michael Stobb)</webMaster><atom:link href="https://stobb.org/thursday_forum/index.xml" rel="self" type="application/rss+xml"/><item><title>Lecture 01: How did we get here?</title><link>https://stobb.org/thursday_forum/2026_lecture_01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>michael@stobb.org (Michael Stobb)</author><guid>https://stobb.org/thursday_forum/2026_lecture_01/</guid><description>&lt;p&gt;This page contains notes, further readings, and general references for the associated lecture.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://docs.google.com/presentation/d/e/2PACX-1vTLaVcNoa7dHYlBRmaX_-6Ynie8RDWHb989P8d8UkfsoOKQzYuYGMlIydZMDcED1MC_ua6ypJwSd8TQ/pub?start=false&amp;amp;loop=false&amp;amp;delayms=60000"&gt;Lecture Slides&lt;/a&gt; for the talk are also available (use your arrow keys to advance the slides) or download a &lt;a href="Stobb_ThursdayForum_2026_Lecture_01.pdf"&gt;pdf of the slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re looking for the other lecture notes, head &lt;a href="https://stobb.org/thursday_forum/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="ai-scaling-laws"&gt;AI Scaling Laws&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A great visualization of the current &lt;a href="https://www.reuters.com/graphics/USA-ECONOMY/AI-INVESTMENT/gkvlqbgxkpb/"&gt;AI investment&lt;/a&gt; is available by Reuters. Be sure to keep scrolling through the page to get a real sense for the current size of the AI investment.&lt;/p&gt;</description></item><item><title>Lecture 02: How do Large Language Models work?</title><link>https://stobb.org/thursday_forum/2026_lecture_02/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>michael@stobb.org (Michael Stobb)</author><guid>https://stobb.org/thursday_forum/2026_lecture_02/</guid><description>&lt;p&gt;This page contains notes, further readings, and general references for the associated lecture.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://docs.google.com/presentation/d/e/2PACX-1vRrzQcgdPw7pqnBPIb9xKzEOC3oJ2ounvZsk7lVo2pDhTbAJazZ63-KFlPVNt8_UgBgrw-GA-aUvfoD/pub?start=false&amp;amp;loop=false&amp;amp;delayms=10000"&gt;Lecture Slides&lt;/a&gt; for the talk are also available (use your arrow keys to advance the slides) or download a &lt;a href="Stobb_ThursdayForum_2026_Lecture_02.pdf"&gt;pdf of the slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re looking for notes to the other lectures, head &lt;a href="https://stobb.org/thursday_forum/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="more-about-perceptrons"&gt;More about Perceptrons&lt;/h2&gt;
&lt;p&gt;While we discussed perceptrons last week, we finally get to see their full power this week once we allow them to have multiple layers. There are tons of great explanations for them online, but my personal favorite is the series by YouTuber &lt;a href="https://www.3blue1brown.com/topics/neural-networks"&gt;3Blue1Brown&lt;/a&gt;. The entire series is worth a watch, but be warned, it&amp;rsquo;s quite long.&lt;/p&gt;</description></item><item><title>Lecture 03: What are the impacts of AI?</title><link>https://stobb.org/thursday_forum/2026_lecture_03/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>michael@stobb.org (Michael Stobb)</author><guid>https://stobb.org/thursday_forum/2026_lecture_03/</guid><description>&lt;p&gt;This page contains notes, further readings, and general references for the associated lecture.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://docs.google.com/presentation/d/e/2PACX-1vT2QczCydIW1pJj9SD5FgZPSd7S91Qnt0i5Id8dlxoRb-9P0rhO70L13dZEc6gmM6-gcByXhoMIktVm/pub?start=false&amp;amp;loop=false&amp;amp;delayms=10000"&gt;Lecture Slides&lt;/a&gt; for the talk are also available (use your arrow keys to advance the slides) or download a &lt;a href="Stobb_ThursdayForum_2026_Lecture_03.pdf"&gt;pdf of the slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re looking for notes to the other lectures, head &lt;a href="https://stobb.org/thursday_forum/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="reliability-and-hallucinations"&gt;Reliability and Hallucinations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We spent a good portion of the first hour discussing why models lie. The technical term is &amp;ldquo;hallucination,&amp;rdquo; and it stems from the probabilistic nature of the technology (as discussed in our previous lecture). A recent report by UX Tigers titled &lt;a href="https://www.uxtigers.com/post/ai-hallucinations"&gt;AI Hallucinations on the Decline&lt;/a&gt; gives a great breakdown of the current error rates (between 0.7% and 3%) for the major frontier models.&lt;/p&gt;</description></item><item><title>Lecture 04: How do we use modern AI tools?</title><link>https://stobb.org/thursday_forum/2026_lecture_04/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>michael@stobb.org (Michael Stobb)</author><guid>https://stobb.org/thursday_forum/2026_lecture_04/</guid><description>&lt;p&gt;This page contains notes, further readings, and general references for the associated lecture.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://docs.google.com/presentation/d/e/2PACX-1vR9akbIFHZj1f5ciFk8q10MSIfdIgWXhtIvwOGRVO_CcQZ5H1XnJ7aTa5LPMLT5bKZ7UmLtDXZxAnQK/pub?start=false&amp;amp;loop=false&amp;amp;delayms=10000"&gt;Lecture Slides&lt;/a&gt; for the talk are also available (use your arrow keys to advance the slides) or download a &lt;a href="Stobb_ThursdayForum_2026_Lecture_04.pdf"&gt;pdf of the slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re looking for notes to the other lectures, head &lt;a href="https://stobb.org/thursday_forum/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="ai-energy-and-water-use"&gt;AI Energy and Water Use&lt;/h2&gt;
&lt;p&gt;Lot&amp;rsquo;s of different numbers were quoted in lecture. Here are some of the sources I pulled those numbers from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/blog/products/infrastructure/measuring-the-environmental-impact-of-ai-inference/"&gt;Google Gemini energy usage&lt;/a&gt; from &lt;em&gt;Google&lt;/em&gt;, so good for transparency but you have to trust the source&lt;/li&gt;
&lt;li&gt;MIT Tech Review article on &lt;a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/"&gt;AI Energy Use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;General energy consumption information from the US Energy Information Administration (EIA) on &lt;a href="https://www.eia.gov/tools/faqs/faq.php?id=1174&amp;amp;t=1"&gt;AC&lt;/a&gt; and &lt;a href="https://www.eia.gov/consumption/manufacturing/data/2022/pdf/Table1_1.pdf"&gt;lots of other stuff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An excellent report on the energy consumption for &lt;a href="https://www.eia.gov/consumption/manufacturing/data/2022/pdf/Table1_1.pdf"&gt;Data Centers&lt;/a&gt; with the breakdown of energy compute vs. HVAC and the different cooling techniques used.&lt;/li&gt;
&lt;li&gt;Goldman Sachs article on the &lt;a href="https://www.goldmansachs.com/insights/articles/why-ai-companies-may-invest-more-than-500-billion-in-2026"&gt;estimated spending on AI for 2026&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wikipedia page on &lt;a href="https://en.wikipedia.org/wiki/Jevons_paradox"&gt;Jevons Paradox&lt;/a&gt; is quite good for getting a basic understanding of it&lt;/li&gt;
&lt;li&gt;Water consumption estimates for &lt;a href="https://water.usgs.gov/edu/activity-watercontent.php"&gt;various things&lt;/a&gt; from the USGS and &lt;a href="https://tatnuckmeatandsea.com/fresh-meat/beef/how-much-beef-do-americans-eat/"&gt;hamburger&lt;/a&gt; consumption estimates for the US&lt;/li&gt;
&lt;li&gt;Good &lt;a href="https://joshuaheathscott.substack.com/p/you-dont-actually-care-about-the"&gt;article&lt;/a&gt; summarizing the state of AI and water consumption&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="ai-and-copyright"&gt;AI and Copyright&lt;/h2&gt;
&lt;p&gt;The copyright fight is a complex issue and I am not a lawyer, but here&amp;rsquo;s a few references for you to read more about the various lawsuits and some of their complexities:&lt;/p&gt;</description></item></channel></rss>