<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 03: What are the impacts of AI? | Michael T. Stobb</title><meta name=keywords content><meta name=description content="This page contains notes, further readings, and general references for the associated lecture.
The Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.
If you&rsquo;re looking for notes to the other lectures, head here.
Reliability and Hallucinations


We spent a good portion of the first hour discussing why models lie. The technical term is &ldquo;hallucination,&rdquo; and it stems from the probabilistic nature of the technology (as discussed in our previous lecture). A recent report by UX Tigers titled AI Hallucinations on the Decline gives a great breakdown of the current error rates (between 0.7% and 3%) for the major frontier models."><meta name=author content="map[email:michael@stobb.org name:Michael Stobb]"><link rel=canonical href=https://stobb.org/thursday_forum/2026_lecture_03/><link crossorigin=anonymous href=/assets/css/stylesheet.960f1449777f1c27112a84b443014ca324d51065a68183f9694cf5e1f9f1f49a.css integrity="sha256-lg8USXd/HCcRKoS0QwFMoyTVEGWmgYP5aUz14fnx9Jo=" rel="preload stylesheet" as=style><link rel=icon href=https://stobb.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://stobb.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://stobb.org/favicon-32x32.png><link rel=apple-touch-icon href=https://stobb.org/apple-touch-icon.png><link rel=mask-icon href=https://stobb.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://stobb.org/thursday_forum/2026_lecture_03/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><meta property="og:url" content="https://stobb.org/thursday_forum/2026_lecture_03/"><meta property="og:site_name" content="Michael T. Stobb"><meta property="og:title" content="Lecture 03: What are the impacts of AI?"><meta property="og:description" content="This page contains notes, further readings, and general references for the associated lecture.
The Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.
If you’re looking for notes to the other lectures, head here.
Reliability and Hallucinations We spent a good portion of the first hour discussing why models lie. The technical term is “hallucination,” and it stems from the probabilistic nature of the technology (as discussed in our previous lecture). A recent report by UX Tigers titled AI Hallucinations on the Decline gives a great breakdown of the current error rates (between 0.7% and 3%) for the major frontier models."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="thursday_forum"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lecture 03: What are the impacts of AI?"><meta name=twitter:description content="This page contains notes, further readings, and general references for the associated lecture.
The Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.
If you&rsquo;re looking for notes to the other lectures, head here.
Reliability and Hallucinations


We spent a good portion of the first hour discussing why models lie. The technical term is &ldquo;hallucination,&rdquo; and it stems from the probabilistic nature of the technology (as discussed in our previous lecture). A recent report by UX Tigers titled AI Hallucinations on the Decline gives a great breakdown of the current error rates (between 0.7% and 3%) for the major frontier models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"2026 Thursday Forum References and Notes","item":"https://stobb.org/thursday_forum/"},{"@type":"ListItem","position":2,"name":"Lecture 03: What are the impacts of AI?","item":"https://stobb.org/thursday_forum/2026_lecture_03/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 03: What are the impacts of AI?","name":"Lecture 03: What are the impacts of AI?","description":"This page contains notes, further readings, and general references for the associated lecture.\nThe Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.\nIf you\u0026rsquo;re looking for notes to the other lectures, head here.\nReliability and Hallucinations We spent a good portion of the first hour discussing why models lie. The technical term is \u0026ldquo;hallucination,\u0026rdquo; and it stems from the probabilistic nature of the technology (as discussed in our previous lecture). A recent report by UX Tigers titled AI Hallucinations on the Decline gives a great breakdown of the current error rates (between 0.7% and 3%) for the major frontier models.\n","keywords":[],"articleBody":"This page contains notes, further readings, and general references for the associated lecture.\nThe Lecture Slides for the talk are also available (use your arrow keys to advance the slides) or download a pdf of the slides.\nIf you’re looking for notes to the other lectures, head here.\nReliability and Hallucinations We spent a good portion of the first hour discussing why models lie. The technical term is “hallucination,” and it stems from the probabilistic nature of the technology (as discussed in our previous lecture). A recent report by UX Tigers titled AI Hallucinations on the Decline gives a great breakdown of the current error rates (between 0.7% and 3%) for the major frontier models.\nTo understand why models sometimes lie to please you (Sycophancy), the researchers at Anthropic (the makers of Claude) released a fascinating paper titled Towards Understanding Sycophancy in Language Models. It is a bit academic, but the abstract and introduction are very readable and explain why an AI might agree with a user’s wrong opinion just to be “helpful.”\nHere’s a great article from TIME focusing on the issues of AI sycophancy\nGeorgeTown Law has compiled a large list of examples where AI sycophancy has proved harmful, and open questions that researchers still need to deal with to make AI models safer.\nSynthetic Media Generation We looked at the “Blind Test” experiment conducted by fantasy author Mark Lawrence. You can read his full blog post here where he details how audiences rated AI-generated stories higher than human ones. One suspected reason for this is that the AI is able to perfectly hit the “average” tropes the readers liked to see, whereas the human authors didn’t quite hit things correctly.\nThe “Grandparent Scam” has evolved significantly. McAfee released a 2025 Report on AI Fraud detailing how voice cloning technology has lowered the barrier for entry for scammers.\nRemember, as discussed in our lecture, please establish a “Safe Word” with your family. This should be a word or phrase that you never share on social media. If you receive a distressed call from a loved one, ask for the word.\nEducation While the “Calculator for Thought” analogy of AI is powerful, it maybe doesn’t capture the full scope. Still, it shows some promise. For a look at one positive potential of AI tutors, Khan Academy’s Khanmigo is a very reasonable and thought provoking implementation. They have excellent demo videos showing how the AI guides students rather than just giving answers.\nOn the flip side, MIT Media lab put out a pretty damning article how using AI tools early can literaly atrophy cognitive skills. Good news though, the group that started without AI tools, learned, then later adopted them did the overall best, which might be the best way forward.\nAnother interesting article (and quite recent) was put out by University World News examining the results of a nationwide survey of educators on the impact of AI on their students. This is less scientific but helps capture the feeling of most educators I have spoken with.\nBiology The 2024 Nobel Prize in Chemistry was awarded to Demis Hassabis and John Jumper of Google DeepMind for AlphaFold. You can read the official press release from the Nobel Committee which explains the significance of solving the “protein folding problem”, a 50-year grand challenge in biology that AI solved in roughly 18 months.\nThe original Nature paper detailing the release of AlphaFold\nA full review article of the protein folding story appeared in Quanta and is just a fantastic read. Highly recommended.\nEmployment We referenced the Goldman Sachs report “How Will AI Affect the Global Workforce?” during our lecture. It is heavy on economics, but the charts are very clear. Robotics You can see the Figure 03 robot launch video where it demonstrates how it can fold clothes, put away dishes, and clean up a room. It’s definitely not perfect (see this video by Time for a more realistic take), but the speed of progress is undeniable. Consciousness and Agency We ended on a philosophical note. The debate on whether these models are “conscious” or just “mimicking consciousness” is ongoing. The research paper we discussed, written by researches at Anthropic, give us some surprising early signs that these models can indeed be introspective about their own internal states, which is a common part of most definitions of “consciousness”. ","wordCount":"726","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":{"email":"michael@stobb.org","name":"Michael Stobb"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://stobb.org/thursday_forum/2026_lecture_03/"},"publisher":{"@type":"Organization","name":"Michael T. Stobb","logo":{"@type":"ImageObject","url":"https://stobb.org/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://stobb.org/ accesskey=h title="Michael T. Stobb (Alt + H)">Michael T. Stobb</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://stobb.org/about/ title=About><span>About</span></a></li><li><a href=https://stobb.org/research/ title=Research><span>Research</span></a></li><li><a href=https://stobb.org/teaching/ title=Teaching><span>Teaching</span></a></li><li><a href=https://stobb.org/software/ title=Software><span>Software</span></a></li><li><a href=https://stobb.org/personal/ title=Personal><span>Personal</span></a></li><li><a href=https://stobb.org/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Lecture 03: What are the impacts of AI?</h1></header><div class=post-content><p>This page contains notes, further readings, and general references for the associated lecture.</p><p>The <a href="https://docs.google.com/presentation/d/e/2PACX-1vT2QczCydIW1pJj9SD5FgZPSd7S91Qnt0i5Id8dlxoRb-9P0rhO70L13dZEc6gmM6-gcByXhoMIktVm/pub?start=false&amp;loop=false&amp;delayms=10000">Lecture Slides</a> for the talk are also available (use your arrow keys to advance the slides) or download a <a href=Stobb_ThursdayForum_2026_Lecture_03.pdf>pdf of the slides</a>.</p><p>If you&rsquo;re looking for notes to the other lectures, head <a href=/thursday_forum/>here</a>.</p><h2 id=reliability-and-hallucinations>Reliability and Hallucinations<a hidden class=anchor aria-hidden=true href=#reliability-and-hallucinations>#</a></h2><ul><li><p>We spent a good portion of the first hour discussing why models lie. The technical term is &ldquo;hallucination,&rdquo; and it stems from the probabilistic nature of the technology (as discussed in our previous lecture). A recent report by UX Tigers titled <a href=https://www.uxtigers.com/post/ai-hallucinations>AI Hallucinations on the Decline</a> gives a great breakdown of the current error rates (between 0.7% and 3%) for the major frontier models.</p></li><li><p>To understand <em>why</em> models sometimes lie to please you (Sycophancy), the researchers at Anthropic (the makers of Claude) released a fascinating paper titled <a href=https://arxiv.org/abs/2310.13548>Towards Understanding Sycophancy in Language Models</a>. It is a bit academic, but the abstract and introduction are very readable and explain why an AI might agree with a user&rsquo;s wrong opinion just to be &ldquo;helpful.&rdquo;</p></li><li><p>Here&rsquo;s a great article from <a href=https://time.com/7346052/problem-ai-flattering-us/>TIME</a> focusing on the issues of AI sycophancy</p></li><li><p>GeorgeTown Law has compiled a large <a href=https://www.law.georgetown.edu/tech-institute/research-insights/insights/ai-sycophancy-impacts-harms-questions/>list</a> of examples where AI sycophancy has proved harmful, and open questions that researchers still need to deal with to make AI models safer.</p></li></ul><h2 id=synthetic-media-generation>Synthetic Media Generation<a hidden class=anchor aria-hidden=true href=#synthetic-media-generation>#</a></h2><ul><li><p>We looked at the &ldquo;Blind Test&rdquo; experiment conducted by fantasy author Mark Lawrence. You can read his full blog post <a href=https://mark---lawrence.blogspot.com/2025/08/the-ai-vs-authors-results-part-2.html>here</a> where he details how audiences rated AI-generated stories higher than human ones. One suspected reason for this is that the AI is able to perfectly hit the &ldquo;average&rdquo; tropes the readers liked to see, whereas the human authors didn&rsquo;t quite hit things correctly.</p></li><li><p>The &ldquo;Grandparent Scam&rdquo; has evolved significantly. McAfee released a <a href=https://www.mcafee.com/ai/news/ai-voice-scam/>2025 Report on AI Fraud</a> detailing how voice cloning technology has lowered the barrier for entry for scammers.</p></li><li><p>Remember, as discussed in our lecture, please establish a &ldquo;Safe Word&rdquo; with your family. This should be a word or phrase that you never share on social media. If you receive a distressed call from a loved one, ask for the word.</p></li></ul><h2 id=education>Education<a hidden class=anchor aria-hidden=true href=#education>#</a></h2><ul><li><p>While the &ldquo;Calculator for Thought&rdquo; analogy of AI is powerful, it maybe doesn&rsquo;t capture the full scope. Still, it shows some promise. For a look at one positive potential of AI tutors, Khan Academy’s <a href=https://khanmigo.ai/>Khanmigo</a> is a very reasonable and thought provoking implementation. They have excellent <a href="https://www.youtube.com/watch?v=rnIgnS8Susg">demo videos</a> showing how the AI guides students rather than just giving answers.</p></li><li><p>On the flip side, MIT Media lab put out a pretty damning <a href=https://arxiv.org/pdf/2506.08872>article</a> how using AI tools early can literaly atrophy cognitive skills. Good news though, the group that started without AI tools, learned, then <em>later</em> adopted them did the overall best, which might be the best way forward.</p></li><li><p>Another interesting article (and quite recent) was put out by <a href="https://www.universityworldnews.com/post.php?story=20260128145305278">University World News</a> examining the results of a nationwide survey of educators on the impact of AI on their students. This is less scientific but helps capture the <em>feeling</em> of most educators I have spoken with.</p></li></ul><h2 id=biology>Biology<a hidden class=anchor aria-hidden=true href=#biology>#</a></h2><ul><li><p>The 2024 Nobel Prize in Chemistry was awarded to Demis Hassabis and John Jumper of Google DeepMind for <strong>AlphaFold</strong>. You can read the official press release from the <a href=https://www.nobelprize.org/prizes/chemistry/2024/press-release/>Nobel Committee</a> which explains the significance of solving the &ldquo;protein folding problem&rdquo;, a 50-year grand challenge in biology that AI solved in roughly 18 months.</p></li><li><p>The original Nature paper detailing the release of <a href=https://www.nature.com/articles/s41586-021-03819-2>AlphaFold</a></p></li><li><p>A full review article of the protein folding story appeared in <a href=https://www.quantamagazine.org/how-ai-revolutionized-protein-science-but-didnt-end-it-20240626/>Quanta</a> and is just a fantastic read. Highly recommended.</p></li></ul><h2 id=employment>Employment<a hidden class=anchor aria-hidden=true href=#employment>#</a></h2><ul><li>We referenced the Goldman Sachs report <a href=https://www.goldmansachs.com/insights/articles/how-will-ai-affect-the-global-workforce>&ldquo;How Will AI Affect the Global Workforce?&rdquo;</a> during our lecture. It is heavy on economics, but the charts are very clear.</li></ul><h2 id=robotics>Robotics<a hidden class=anchor aria-hidden=true href=#robotics>#</a></h2><ul><li>You can see the <a href="https://youtu.be/Eu5mYMavctM?si=7yZ9PacOL4emaTFa">Figure 03 robot</a> launch video where it demonstrates how it can fold clothes, put away dishes, and clean up a room. It&rsquo;s definitely not perfect (see this video by <a href="https://youtu.be/4ZP943-gARQ?si=LDi_aKQjFAep43TG">Time</a> for a more realistic take), but the speed of progress is undeniable.</li></ul><h2 id=consciousness-and-agency>Consciousness and Agency<a hidden class=anchor aria-hidden=true href=#consciousness-and-agency>#</a></h2><ul><li>We ended on a philosophical note. The debate on whether these models are &ldquo;conscious&rdquo; or just &ldquo;mimicking consciousness&rdquo; is ongoing. The research <a href=https://transformer-circuits.pub/2025/introspection/index.html>paper</a> we discussed, written by researches at Anthropic, give us some surprising early signs that these models can <em>indeed</em> be introspective about their own internal states, which is a common part of most definitions of &ldquo;consciousness&rdquo;.</li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://stobb.org/>Michael T. Stobb</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>